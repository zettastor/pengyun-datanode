# you can set localhost:10011, the out system will select a default ip address to constructor a endpoint,
# and you can set a ip address for main endpoint or heartbeat endpoint
app.data.tranfer.endpoint=30066
dih.endpoint=10000
thrift.client.timeout=20000
health.checker.rate=1000
start.page.flusher=true
log.persist.root.dir=var/storage/datalog
shutdown.save.log.dir=var/storage/savelog
max.wait.for.datanode.shutdown.time.second=480
# e.g. we create 5000 segmentUnits in one data node at most, 5000 logs in one file is 100KB size with each log is 20bytes, 
# we keep 25000 files in whole data node, it means we need 100KB*25000 = 2.5G capacity at least for data log storage
num.logs.in.one.file=5000
num.files.in.datanode=25000
# the total log file count is decided by the {num.files.in.datanode} and {num.files.in.one.segment.unit}. now we should
# we can create 5000 segment units in one data node at most, only 3 log files can be left every segment unit, it means we 
# need 100KB * 5000 * 3 = 1.5G capacity at least for data log storage.
num.files.in.one.segment.unit=200
testing=false
instance.id.for.testing=10011
###how long should a new segment unit request be sent since the last request
request.new.segment.unit.expiration.threshold.ms=240000
###how long we have to wait to recruit a new secondary
threshold.to.request.for.new.member.ms=1800000
###the interval for a segment unit to move from Deleting to Deleted status 
wait.time.ms.to.move.segment.to.deleted=300000
###the rate of executing segmentUnitDelete
segmentunitdeleter.execution.rate.ms=60000
###the interval for broken segment to deleted
wait.time.ms.from.broken.to.deleted=0
###janitor is responsible for there is an inactive secondaries whose lease or heart beat is timeout
###janitor.execution.rate.ms is the period of this thread
janitor.execution.rate.ms=300000
### connections per datanode endpoint should have been deprecated
max.async.connections.per.datanode.endpoint=50
max.sync.connections.per.datanode.endpoint=50
### the size of the memory for data logs
archive.report.rate.ms=5000
segment.report.rate.ms=10000
page.checksum.algorithm=CRC32
network.checksum.algorithm=DIGEST
#config sector size
fast.buffer.allocate.aligned=9
#disk check rate
storage.check.rate.ms=60000
### when archive in offling status,need to check rollback if some segment units can not heartbeat others
need.check.rollback.in.offling=true
### ***log4j properties*** ###
### Not use anymore. In future, we might need it. Comment it out for now
#log.level=WARN
#log.output.file=logs/datanode.log
write.storage.io.step=4
read.storage.io.step=4
### network thread pool
max.service.workers=1000
### max size per IO request, the value should be less the 16M, because the default max frame size of network is 16M
max.io.data.size.per.request.bytes=1048576
max.io.pending.requests=1000
### max network pending size for some channel
max.channel.pending.size.mb=1024
### percentage of fast buffer for primary, all these three percentage can not be large than 100
primary.fast.buffer.percentage=30
secondary.fast.buffer.percentage=60
sync.log.fast.buffer.percentage=10
# create log for waiting max time
wait.for.memory.buffer.timeout.ms=50
# max number of thread count for PCL driver thread pool
max.pool.size.for.catchup.log.engine.pcl=40
# max number of thread count for PPL driver thread pool
max.pool.size.for.catchup.log.engine.ppl=20
# the type is string, you can specify the value as 1G, 1024M or 1073741824, this is, you can set the value with
# unit: M/m/G/g/K/k, and set the value without unit, which means that you set unit as byte. if you set the value
# as 0, it will create the memory cache with internal allocate algorithm.
page.system.memory.cache.size=0
#512M
indexer.max.flushers=10
indexer.max.transactions=20000
indexer.flusher.bundle.size=10000
indexer.flusher.bundle.timeout.sec=2
indexer.log.root=var/indexerLog
#10M
indexer.per.log.max.size.bytes=10485760
indexer.max.opened.log.files=100
indexer.max.lazy.finalizations=2000
######
# Options for new indexer
######
#
# Block cache size per archive for indexer. That is total block cache size for indexer is:
# amount_of_archives * indexer_block_cache_size_per_archive
#
# default: 134217728 (128MB)
#
#
# Specify indexer compression type, valid values are:
#
# NO_COMPRESSION (default, compression is disabled)
# SNAPPY_COMPRESSION
# ZLIB_COMPRESSION
# BZLIB2_COMPRESSION
# LZ4_COMPRESSION
# LZ4HC_COMPRESSION
# XPRESS_COMPRESSION
# ZSTD_COMPRESSION
#
# All above values are case insensitive
#
indexer.compression.type=no_compression
max.push.interval.time.ms=60000
max.log.id.window.size=50000
max.log.id.window.split.count=50
# primary can push the number of pages to secondary.
max.copy.page.iops=100000
clone.segment.unit.count.each.time=4
clone.request.count.each.segment.unit=1
clone.page.count.each.request=1
# default.l2.cache.name=nvme0n1
number.of.dirty.page.per.flush.request.for.ssd=32
#enough space for any log persistence 
log.dirs=logs/,var/
min.required.log.size.mb=500
min.reserved.disk.size.mb=200
persist.root.directory=var/storage
target.io.latency.in.datanode.ms=100
ping.host.timeout.ms=50
network.connection.check.delay.ms=10000
network.connection.detect.retry.maxtimes=5
network.connection.detect.server.listening.port=54321
arbiter.count.limit.in.one.archive=500
arbiter.count.limit.in.one.datanode=15000
smart.min.copy.speed.mb=20
smart.max.copy.speed.mb=100
# io average delay statistic rate, time unit is millisecond
io.delay.timeunit=MILLISECONDS
# device ignore by datanode
#ignore.device.name.reg=fd[0-9],zd[0-9]
ignore.device.name.reg=fd[0-9]
raw.app.name=RAW
enable.auto.distribute.disk.usage=true
archive.init.mode=append
use.python.script=true
# must not use decimal number
rocksdb.partition.size=50G
# whether partition ssd disk
need.partition.ssd.disk=false
# create file system cmd
mkfs.cmd=mkfs.ext4
# important path environment, some system may miss some
path.environment=/usr/local/sbin,/usr/local/bin,/usr/sbin,/usr/bin,/sbin,/bin
# default location for rocksdb
rocksdb.default.folder=default
rocksdb.partition.folder=partition
# how much disk that cache reserve area should take
# whether enable disk native cache, if config false will disable cache, if config true will do nothing
enable.disk.native.cache=false
# whether need check os version when start up
need.check.os.version=true
# for cache metadata manager
# cache address transformer reserved bit count for segid
cache.address.transformer.segid.len=22
page.metadata.need.flush.to.disk=true
# alarm report period interval
alarm.report.rate.ms=5000
